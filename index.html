To use gradient descent we need to find the derivatives of the cost function with respect to each of the parameters. To do this, let’s formalize our picture of a neural network using matrices and vectors. The reason for this is that backpropagation is a lot more concise using matrix notation. 

Let’s call the output of any neuron it’s activation, and the vector of all the activations in a layer \( \textbf a^l \) Now this vector will be a \( 1 \times N \) dimensional vector for a layer with N neurons. 

PIC 1

Now let’s figure out how to write the weights in matrix notation. Of course, we could just throw all the weights into some matrix and call it a day, but let’s be a bit more clever about it. In particular, let’s take advantage of the dot product. Let’s define a \( N \times 1 \) dimensional matrix to be the weights matrix from a previous layer to **a single neuron** in the next layer, where \( N \) would be the number of neurons in the **previous** layer (all of this seemingly arbitrary notation will pay off in the end, we promise). Let’s call this matrix \( w^l_k \) for the \( k^{th} \) neuron in the \( l^{th} \) layer.

PIC 2

Notice now that for the \( k^{th} \) neuron in the \( l^{th} \) layer, we can get the \( k^{th} \) weighted sum of its inputs, called \( z^l_k \) (where \( l \) is the layer and \( k \) is the \( k^{th} \) neuron, by simply taking the dot product between \( w^l_k \) and \( a^{l-1} \) 


<script type="text/javascript" async
  src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
